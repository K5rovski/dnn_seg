{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Training Instances\n",
    "\n",
    "\n",
    "The instances whic\n",
    "\n",
    "This module deals with saving all training/validation instances -- for ease of convinience.\n",
    "\n",
    "Using different options:\n",
    "+ image masks -- Training instances;\n",
    "+ uniform sampling parameters -- Validation Instances.\n",
    "\n",
    "**Image patches** (used as input/output for the neural networks). are saved to disk \n",
    "\n",
    "+ Instances are automatically saved in a **random**, **batched** way. -- For running the experiment, using a smaller sized RAM Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules\n",
    "\n",
    "Here we import the needed python functions for sampling ROIs.\n",
    "\n",
    "+ The database used was [LMDB](https://lmdb.readthedocs.io/).\n",
    "+ The data are saved in a key-value pair base, with randomly assigned keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every import is succesful !\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import dnn_segmentation\n",
    "\n",
    "#SETUPP    ============================\n",
    "from  dnn_segmentation.data_prep.utils.create_lmdb_batch_funcs import   \\\n",
    "        reduce_files,make_db_folder,create_lmdbs\n",
    "\n",
    "from  dnn_segmentation.data_prep.utils.data_helper import create_chessmask\n",
    "\n",
    "from dnn_segmentation.net.utils.models import get_2k_image_2layer_convnetmodel\n",
    "from dnn_segmentation.net.utils.train_h import save_relevant\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "print('Every import is succesful !')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Configutration Experiment Files\n",
    "\n",
    "This code block is used to save the database step -- configuration text files.\n",
    "\n",
    "**For experiment recreation, this saved configuration text file has all needed info to reproduce the same output.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_funcs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3954b48b1974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mquick_str\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'base_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m97\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m97\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m26\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m files_save=[ os.path.join('data_funcs',f) for f in os.listdir('data_funcs')              if os.path.isfile(os.path.join('data_funcs',f))\n\u001b[0m\u001b[1;32m      6\u001b[0m              and not os.path.join('data_funcs',f).endswith('.pyc')]\n\u001b[1;32m      7\u001b[0m \u001b[0mfiles_save\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_funcs'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "np.random.seed(None)\n",
    "quick_str='base_'+''.join(map(chr,np.random.randint(97,97+26,(5,))) )\n",
    "files_save=[]\n",
    "files_save.insert(0,os.path.basename(__file__))\n",
    "\n",
    "conf_test_tosave=save_relevant('../data/saved_base_confs',quick_str,\n",
    "            files=files_save,\n",
    "            just_return=True)\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting parameters\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DESCRIPTION_TEXT='normal sample, interim ON, 8 mil, specific images and sampling, 61window 3x3 grid'\n",
    "\n",
    "db_folder=r'D:\\data_projects\\neuron_fibers_data\\BASES\\8mil_61_bigp_Wsize_20imgs_interimONset'\n",
    "\n",
    "save_base=True\n",
    "image_width,image_height=61,61\n",
    "big_image_size=2400\n",
    "lookup_path = r'D:\\data_projects\\neuron_fibers_data\\images\\ML_SET_ON01\\cints_new_general'\n",
    "\n",
    "groundtruth_path = r'D:\\data_projects\\neuron_fibers_data\\images\\cors\\c'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "val_images='sp14252-img06,sp14436-img05,sp14436-img04,sp14252-img08'.split(',')\n",
    "\n",
    "# Interim mlset02\n",
    "val_images='sp13878-img05,sp13933-img02,sp14105-img03,sp14436-img03'.split(',')\n",
    "\n",
    "#interim rand\n",
    "# val_images=['sp13909-img05', 'sp14252-img01', 'sp13878-img05', 'sp13938-img01', 'sp14252-img07']\n",
    "\n",
    "# mlset ON\n",
    "val_images=['sp13726-img09', 'sp14245-img06', 'sp14484-img01', 'sp13750-img01']\n",
    "\n",
    "\n",
    "test_image_val=[filename for ind,filename in enumerate(glob.glob(os.path.join(lookup_path, '*.tif')) ) \\\n",
    "if  filename[filename.rindex(os.path.sep)+1:filename.rindex('img')+5] in val_images ]\n",
    "\n",
    "val_do_expand=False\n",
    "percent_patches_used_val=np.array([1,1,1])*0.1  #np.array([0.837,0.744,1])*0.1 #[4.0/19,7.0/10,1]\n",
    "phase_done_val='val'\n",
    "val_batch_size=1*10**5\n",
    "val_lmdb_GBsize=80\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Train Parameters'''\n",
    "\n",
    "train_mask_dir=r'D:\\data_projects\\neuron_fibers_data\\images\\patch_mask_ON_chosenTrainingImgs\\\\'\n",
    "\n",
    "train_images='''sp14436-img08,sp13909-img01,sp13909-img02,sp13938-img07,\n",
    "sp13726-img01,sp14252-img05,sp13909-img07,sp13909-img08,sp13909-img03,\n",
    "sp13933-img04,sp13726-img04,sp13726-img03,sp13933-img08,sp13909-img05,\n",
    "sp13726-img02,sp13933-img05,sp13933-img06,sp13933-img07,sp13909-img06,\n",
    "sp13726-img08'''.split(',')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_image_train=[filename for ind,filename in enumerate(glob.glob(os.path.join(lookup_path, '*.tif')) )\\\n",
    "\n",
    " if  filename[filename.rindex(os.path.sep)+1:filename.rindex('img')+5] in train_images ]\n",
    "\n",
    "train_do_expand=False\n",
    "percent_patches_used_train= np.array([1,1,1])*0.65#*(1.0/8)  #[4.0/19,7.0/10,1]\n",
    "phase_done_train='train'\n",
    "\n",
    "train_batch_size=1*10**5\n",
    "train_lmdb_GBsize=350\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "do_special={'perc_switch':0,'not_mix_patches':True,\n",
    "            'two_patch':True,'folded_expand':True}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not used parameters \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#!!!!!!!!!!!!NOT NEEDED ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "image_list_folder='#########'\n",
    "image_prepend='###########'\n",
    "save_path = '#################'\n",
    "patch_dir='##################'\n",
    "save_path = '#################'\n",
    "# =============================================\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the database saving process\n",
    "\n",
    "The two(train and val) image sets are iterated,\n",
    "and saved in a **batched** way.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "start_time = time.time()\n",
    "print ('Creating images at \"%s\" ...' % db_folder)\n",
    "print( 'Working on: ',len(test_image_val),test_image_val)\n",
    "\n",
    "make_db_folder(db_folder)\n",
    "\n",
    "create_lmdbs(db_folder,    phase_done_val,   \n",
    "             (test_image_val,lookup_path, groundtruth_path, \n",
    "                  \"all_test\", 0, save_path),\n",
    "             image_list_folder,image_prepend,image_width,\n",
    "             image_height,smaller_size=percent_patches_used_val,\n",
    "             random_key_prepend=12,do_expand=val_do_expand,\n",
    "             patch_size=image_width,save_base=save_base,\n",
    "             batch_size=val_batch_size,base_GBsize=val_lmdb_GBsize,\n",
    "            do_special=do_special\n",
    ")\n",
    "reduce_files(db_folder,phase_done_val)\n",
    "\n",
    "print( 'Done after {:.2f} hours'.format((time.time() - start_time)/3600 ))\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print ('Working on: ',len(test_image_train),test_image_train)\n",
    "\n",
    "create_lmdbs(db_folder,    phase_done_train,   \n",
    "             (test_image_train,lookup_path, groundtruth_path,\n",
    "                      \"all_test\", 0, save_path)\n",
    "             ,image_list_folder,image_prepend,image_width,\n",
    "             image_height,smaller_size=percent_patches_used_train,\n",
    "             mean_name='mean.jpg',random_key_prepend=12,\n",
    "             do_expand=train_do_expand,patch_size=image_width,\n",
    "             save_base=save_base,batch_size=train_batch_size,\n",
    "             mask_dir=train_mask_dir,base_GBsize=train_lmdb_GBsize,\n",
    "           do_special=do_special\n",
    "            )\n",
    "reduce_files(db_folder,phase_done_train)\n",
    "\n",
    "print( 'Done after {:.2f} hours'.format((time.time() - start_time)/3600 ))\n",
    "\n",
    "# ====================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually saving the configuration info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "\n",
    "save_relevant('data_funcs/saved_baseconfs',quick_str,str_to_save=conf_test_tosave,descriptive_text=DESCRIPTION_TEXT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
